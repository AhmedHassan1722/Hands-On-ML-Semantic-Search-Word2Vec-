# -*- coding: utf-8 -*-
"""Word2vec_Task.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QRQ-UNg1OrSAZ-uQCK0SY60SBIaA7jOA
"""

!pip install gensim PyPDF2 nltk

!pip install gdown
!gdown --id 0B7XkCwpI5KDYNlNUTTlSS21pQmM

from gensim.models import KeyedVectors

model = KeyedVectors.load_word2vec_format(
    "GoogleNews-vectors-negative300.bin.gz",
    binary=True
)
print("Model loaded!")



!pip install gradio gensim nltk scikit-learn PyPDF2

import re
import numpy as np
import gradio as gr
import PyPDF2
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from gensim.models import Word2Vec
from numpy.linalg import norm

# -----------------------------
# 1Ô∏è‚É£ Preprocessing functions
# -----------------------------
stop_words = set(stopwords.words("english"))

def preprocess(text):
    text = text.lower()
    text = re.sub(r"[^a-zA-Z\s]", " ", text)
    tokens = word_tokenize(text)
    tokens = [w for w in tokens if w not in stop_words and len(w) > 2]
    return tokens

def tfidf_w2v_vector(tokens, model, tfidf_vocab, idf_scores):
    vectors = []
    weights = []

    for token in tokens:
        if token in model.key_to_index and token in tfidf_vocab:
            vec = model[token]
            weight = idf_scores[tfidf_vocab[token]]
            vectors.append(vec * weight)
            weights.append(weight)

    if not vectors:
        return np.zeros(model.vector_size)

    return np.sum(vectors, axis=0) / np.sum(weights)

def cosine_sim(a, b):
    if norm(a) == 0 or norm(b) == 0:
        return 0
    return np.dot(a, b) / (norm(a) * norm(b))

def chunk_text(text, chunk_size=100):
    words = text.split()
    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]

# -----------------------------
# 2Ô∏è‚É£ Read PDF
# -----------------------------
def read_pdf(path):
    text = ""
    with open(path, "rb") as f:
        reader = PyPDF2.PdfReader(f)
        for page in reader.pages:
            text += page.extract_text() + "\n"
    return text

# Replace this with the path to your PDF
pdf_path = "/content/Hands On Machine Learning with Scikit Learn and TensorFlow.pdf"
book_text = read_pdf(pdf_path)

# -----------------------------
# 3Ô∏è‚É£ Prepare embeddings
# -----------------------------
chunk_size = 100
chunks = chunk_text(book_text, chunk_size)
processed_chunks = [preprocess(chunk) for chunk in chunks]

# Train Word2Vec
model = Word2Vec(sentences=processed_chunks, vector_size=100, window=5, min_count=1, workers=4).wv

# TF-IDF
chunk_texts = [" ".join(tokens) for tokens in processed_chunks]
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(chunk_texts)
tfidf_vocab = vectorizer.vocabulary_
idf_scores = vectorizer.idf_

# Chunk embeddings
chunk_embeddings = [
    tfidf_w2v_vector(tokens, model, tfidf_vocab, idf_scores)
    for tokens in processed_chunks
]

# -----------------------------
# 4Ô∏è‚É£ Search function
# -----------------------------
def search_top3(query):
    query_tokens = preprocess(query)
    query_embedding = tfidf_w2v_vector(query_tokens, model, tfidf_vocab, idf_scores)

    results = []
    for chunk, emb in zip(chunks, chunk_embeddings):
        sim = cosine_sim(query_embedding, emb)
        results.append((chunk, sim))

    top3 = sorted(results, key=lambda x: x[1], reverse=True)[:3]

    output = ""
    for chunk, sim in top3:
        output += f"üîπ Similarity = {sim:.4f}\n"
        output += "----------------------------------------------------\n"
        output += chunk + "\n"
        output += "----------------------------------------------------\n\n"

    return output

# -----------------------------
# 5Ô∏è‚É£ Gradio interface
# -----------------------------
with gr.Blocks() as interface:
    gr.Markdown("# üìò Hands-On ML Semantic Search (Word2Vec)")
    gr.Markdown("Enter a query and get the top 3 most relevant chunks from the book.")

    query_input = gr.Textbox(label="Enter your question", placeholder="What is machine learning?")
    output_box = gr.Textbox(label="Top 3 Chunks", lines=20)

    search_button = gr.Button("Search")
    search_button.click(fn=search_top3, inputs=query_input, outputs=output_box)

interface.launch()